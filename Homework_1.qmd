---
title: "Homework 1"
format: html
editor: visual
---

## Exercise 1

Read (Sec 1.1, pp1-2 [Sutton and Barto 2018](https://sauldiazinfante.github.io/RL-Course-2024-2/references.html#ref-Sutton2018)) and answer the following.

Explain why Reinforcement Learning differs for supervised and unsupervised learning.

**Answer:**

Reinforcement Learning is different from supervised learning because it doesn't need a training set of examples to "learn". RL shoould learn from it's own experience.

it is also different from unsupervised learning because it isn't trying to find a hidden structure. RL is trying to maximize a reward or minimize a cost.

## Exercise 2

See the first Steve Brunton's youtube video about [Reinforcement Learning](https://www.youtube.com/watch?v=0MNVhXEX9to&list=PLMrJAkhIeNNQe1JXNvaFvURxGY4gE9k74). Then accordingly to its presentation explain what is the meaning of the following expression:

$$
V_\pi (s) = \mathbb{E}\left(\sum_t \gamma^t r_t | s_0 = s\right)
$$

**Answer:**

It is the expected reward if we start in a state $s$ and we enact the policy $\pi$. The $\gamma$ in the equation is a discounting rate.

## Exercise 3

Form (see [Sutton and Barto 2018, sec. 1.7](https://sauldiazinfante.github.io/RL-Course-2024-2/references.html#ref-Sutton2018)) obtain a time line pear year from 1950 to 2012.

Use the following format [see https://kim.quarto.pub/milestones–bar-timelines/](https://kim.quarto.pub/milestones--bar-timelines/)

## Exercise 4

Consider the following **consumption–saving** problem with dynamics

$$
x_{k+1} = (1+r)(x_k-a_k), \ \ k=0,1,\dots , N-1,
$$

and utility function

$$
\beta^N(x_N)^{1-\gamma} + \sum_{k=0}^{N-1}\beta^k(a_k)^{1-\gamma}.
$$

Show that the value functions of the DP algorithm take the form

$$
J_k(x)=A_k\beta^kx^{1-\gamma},
$$

where $A_N =1$ and for $k =N-1,\dots,0,$

$$
A_k =[1+((1+r)\beta A_{k+1})^{1/\gamma}]^\gamma.
$$

Show also that the optimal policies are $h_k(x) = A_k^{-1/\gamma}x$, for $k =N-1,\dots,0$.

**Answer:**

First, let's find the $a$ which minimize the function $J_{N-1}(x)$ .

$$
J_{N-1}(x) = \min_{a\in A}\{ C_{N-1}(x,a) + J_N(f(x,a)) \}
$$

We know, from the function of utility, that $J_N(x) = \beta^{N} (x_N)^{1-\gamma}$ and $J_{N-1}(x) = \beta^{N-1} (a)^{1-\gamma}$, and we also have that $x_{k} = (1+r)(x_k-a_k)$, so

$$
J_{N-1}(x) = \min_{a\in A}\{ \beta^{N-1}a^{1-\gamma} + \beta^N(1+r)^{1-\gamma}(x-a)^{1-\gamma}\}.                  \tag{1}
$$a = \frac{x}{(\beta(1+r)^{1-\gamma})^{1/\gamma}+1}.

Let's get the derivative respect to $a$ and make it equal to 0,

$$
(1-\gamma)\beta^{N-1}a^{-\gamma}-(1-\gamma)\beta^N(1+r)^{1-\gamma}(x-a)^{-\gamma} =0.
$$

Now, we arrange the equation so we can find $a$:

$$
\left( (1-\gamma)\beta^{N-1} \right)\left( a^{-\gamma}-\beta(1+r)^{1-\gamma}(x-a)^{-\gamma}\right) =0
$$

$$
\Longrightarrow \ \ \ \ a^{-\gamma}-\beta(1+r)^{1-\gamma}(x-a)^{-\gamma} =0
$$

$$
\Longrightarrow \ \ \ \ a^{-\gamma}(x-a)^{\gamma} =\beta(1+r)^{1-\gamma}
$$

$$
\Longrightarrow \ \ \ \ \left(\frac{x-a}{a}\right)^{\gamma}=\beta(1+r)^{1-\gamma}
$$

$$
\Longrightarrow \ \ \ \ a = \frac{x}{(\beta(1+r)^{1-\gamma})^{1/\gamma}+1}.
$$

Here $a$ is the action that give us $J_{N-1}(x)$, so we value $a$ on $(1)$,

$$
J_{N-1}(x) =  \beta^{N-1}\left( \frac{x}{(\beta(1+r)^{1-\gamma})^{1/\gamma}+1}\right)^{1-\gamma} + \beta^N(1+r)^{1-\gamma}\left(x-\frac{x}{(\beta(1+r)^{1-\gamma})^{1/\gamma}+1}\right)^{1-\gamma}.
$$

Let $A_{N-1}= ((\beta(1+r)^{1-\gamma})^{1/\gamma}+1)^\gamma$, we can rewrite $J_{N-1}(x)$ as

$$
J_{N-1}(x) =  \beta^{N-1}\left( \frac{x}{A_{N-1}^{1/\gamma}}\right)^{1-\gamma} + \beta^N(1+r)^{1-\gamma}\left(x-\frac{x}{A_{N-1}^{1/\gamma}}\right)^{1-\gamma}
$$

$$
 =  \beta^{N-1}\left( \frac{x}{A_{N-1}^{1/\gamma}}\right)^{1-\gamma} + \beta^N(1+r)^{1-\gamma}\left(\frac{xA_{N-1}^{1/\gamma}-x}{A_{N-1}^{1/\gamma}}\right)^{1-\gamma}
$$

$$
 =  \beta^{N-1}x^{1-\gamma}\left(\left( \frac{1}{A_{N-1}^{1/\gamma}}\right)^{1-\gamma} + \beta(1+r)^{1-\gamma}\left(\frac{A_{N-1}^{1/\gamma}-1}{A_{N-1}^{1/\gamma}}\right)^{1-\gamma}\right)
$$

$$
 =  \beta^{N-1}x^{1-\gamma}\left(\left( \frac{1}{A_{N-1}^{1/\gamma}}\right)^{1-\gamma} + (A_{N-1}^{1/\gamma}-1)^\gamma\left(\frac{A_{N-1}^{1/\gamma}-1}{A_{N-1}^{1/\gamma}}\right)^{1-\gamma}\right)
$$

$$
= \beta^{N-1}x^{1-\gamma}A_{N-1}.
$$

So we have that $J_{N-1}(x) = \beta^{N-1}x^{1-\gamma}A_{N-1}$.

Next, let's proof

## Exercise 5

Consider now the infinite–horizon version of the above consumption–saving problem.

i.  Write down the associated Bellman equation.

    **Answer:**

ii. Argue why a solution to the Bellman equation should be of the form

    $$
    v(x)=cx^{1-\gamma},
    $$

    where $c$ is constant. Find the constant $c$ and the stationary optimal policy.

    **Answer:**

## Exercise 6

Let $\{ \xi_k \}$ be a sequence of iid random variables such that $E[\xi]=0$ and $E[\xi^2]=d$. Consider the dynamics

$$
x_{k+1}=x_k+a_k+\xi_k, \ \ k=0,1,\dots,
$$

and the discounted cost

$$
E\sum \beta^k(a_k^2+x_k^2).
$$

i.  Write down the associated Bellman equation.

ii. Conjecture that the solution to the Bellman equation takes the for

    $v(x) =ax^2+b$, where $a$ and $b$ are constant.

    Determine the constants $a$ and $b$.
